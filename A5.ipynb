{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Warmup\n",
    "As a warmup, write code to collect statistics about word frequencies in the two languages. Print the 10 most frequent words in each language.\n",
    "\n",
    "If you're working with Python, using a CounterLinks to an external site. is probably the easiest solution.\n",
    "\n",
    "Let's assume that we pick a word completely randomly from the European parliament proceedings. According to your estimate, what is the probability that it is speaker? What is the probability that it is zebra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent(file_path):\n",
    "    sent = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences = re.split(r'\\s', line.strip())\n",
    "            for sentance in sentences:\n",
    "                if sentance.strip():\n",
    "                    sent.append(sentance.strip())\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_swe = \"datasets/europarl-v7.sv-en.lc.sv\"\n",
    "swe_sentance = extract_sent(file_path_swe)\n",
    "file_path_eng = \"datasets/europarl-v7.sv-en.lc.en\"\n",
    "eng_sentance = extract_sent(file_path_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".: 9648\n",
      "att: 9181\n",
      ",: 8876\n",
      "och: 7038\n",
      "i: 5949\n",
      "det: 5687\n",
      "som: 5028\n",
      "för: 4959\n",
      "av: 4013\n",
      "är: 3840\n"
     ]
    }
   ],
   "source": [
    "counter_swe = Counter(swe_sentance)\n",
    "most_common_swe = counter_swe.most_common(10)\n",
    "for word, count in most_common_swe:\n",
    "   print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 19322\n",
      ",: 13514\n",
      ".: 9774\n",
      "of: 9312\n",
      "to: 8801\n",
      "and: 6946\n",
      "in: 6090\n",
      "is: 4400\n",
      "that: 4357\n",
      "a: 4269\n"
     ]
    }
   ],
   "source": [
    "counter_eng = Counter(eng_sentance)\n",
    "most_common_eng = counter_eng.most_common(10)\n",
    "for word, count in most_common_eng:\n",
    "   print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of picking 'speaker' randomly: 0.0009008197459688316\n",
      "Probability of picking 'zebra' randomly: 0.0\n",
      "Probability of picking 'the' randomly: 1.7405639131609765\n"
     ]
    }
   ],
   "source": [
    "total_words_eng = len(counter_eng)\n",
    "speaker_counts = counter_eng['speaker']\n",
    "zebra_counts = counter_eng['zebra']\n",
    "the_counts = counter_eng['the']\n",
    "\n",
    "prob_speaker = speaker_counts/total_words_eng\n",
    "prob_zebra = zebra_counts/total_words_eng\n",
    "prob_the= the_counts / total_words_eng \n",
    "\n",
    "print(\"Probability of picking 'speaker' randomly:\", prob_speaker)\n",
    "print(\"Probability of picking 'zebra' randomly:\", prob_zebra)\n",
    "print(\"Probability of picking 'the' randomly:\", prob_the)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Language modeling\n",
    "We will now define a language model that lets us compute probabilities for individual English sentences.\n",
    "\n",
    "Implement a bigram language model as described in the lecture, and use it to compute the probability of a short sentence.\n",
    "What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? And what happens if your sentence is very long (e.g. 100 words or more)? Optionally, change your code so that it can handle these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "1. If a word in the text did not appear, the bigram probability would be 0. This might not accurately reflect the actual likelihood of the sentence occurring in real language usage. There are several ways to solve this, we will use laplace smoothening. This means that instead if assigning 0 probability to an unseen bigram, we will add a pseudo-count to each possible event, ensuring that even unseen bigrams have a non-zero probability.\n",
    "2. For very long sentances, the probability might become a very small number to the muliplication of probabilites. This could lead to that the floating-point arithmetic becomes less precise. To handle this we will use log probabilities instead of probabilities directly, as logarithms can help maintain numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_translation_model(sentences):\n",
    "    \n",
    "    # count all bigrams from the text\n",
    "    bigram_counts = Counter(zip(sentences, sentences[1:]))\n",
    "\n",
    "    # calculate the probabilities\n",
    "    bigram_prob = {}\n",
    "    size = len(set(sentences))\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        prev_word = bigram[0]\n",
    "        bigram_prob[bigram] = np.log((count + 1) / (sentences.count(prev_word) + size))\n",
    "    \n",
    "    return bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentance_log_prob(sentance, bigram_prob):\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(sentance)-1):\n",
    "        bigram = (sentance[i],sentance[i+1])\n",
    "        log_prob += bigram_prob.get(bigram, np.log(1e-10))\n",
    "    return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probabilities = train_translation_model(eng_sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Translation modeling. We will now estimate the parameters of the translation model P(f|e).\n",
    "\n",
    "Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "\n",
    "Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate. It can be interesting to look at this list of 10 words and see how it changes during the EM iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(source, target):\n",
    "    return {(f, e): np.random.rand() for f in source for e in target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_model(source, target, iterations):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
