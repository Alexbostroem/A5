{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Translation modeling. We will now estimate the parameters of the translation model P(f|e).\n",
    "\n",
    "Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "\n",
    "Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate. It can be interesting to look at this list of 10 words and see how it changes during the EM iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013157894736842105\n",
      "0.0967741935483871\n",
      "0.08168535863305357\n",
      "0.0612108085841098\n",
      "0.04675750078441429\n",
      "0.0368707668538915\n",
      "0.030013385320389337\n",
      "0.025152961997690494\n",
      "0.021631448258980935\n",
      "0.019030246860963575\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, n, corpus):\n",
    "        self.n = n\n",
    "        self.ngrams = {}\n",
    "        self.total_ngrams = 0\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def train(self):\n",
    "        for sentence in self.corpus:\n",
    "            for i in range(len(sentence) - self.n + 1):\n",
    "                ngram = tuple(sentence[i:i+self.n])\n",
    "                self.ngrams[ngram] += 1\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "    def probability(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        prob = 1.0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i+self.n])\n",
    "            prob *= self.ngrams[ngram] / self.total_ngrams\n",
    "        return prob\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, swedish_file, english_file):\n",
    "        self.swedish_file = swedish_file\n",
    "        self.english_file = english_file\n",
    "        self.swedish_sentences = []\n",
    "        self.english_sentences = []\n",
    "        self.swedish_vocab = {}\n",
    "        self.english_vocab = {}\n",
    "        self.bigram_probabilities = {}\n",
    "        self.translation_probabilities = {}\n",
    "        self.ngram_order = 2\n",
    "        \n",
    "    def read_files(self):\n",
    "        with open(self.swedish_file, 'r', encoding='utf-8') as f:\n",
    "            self.swedish_sentences = f.readlines()\n",
    "        with open(self.english_file, 'r', encoding='utf-8') as f:\n",
    "            self.english_sentences = f.readlines()\n",
    "    \n",
    "    def load_subset(self, num_sentences):\n",
    "        with open(self.swedish_file, 'r', encoding='utf-8') as f:\n",
    "            self.swedish_sentences = [next(f).strip() for _ in range(num_sentences)]\n",
    "        with open(self.english_file, 'r', encoding='utf-8') as f:\n",
    "            self.english_sentences = [next(f).strip() for _ in range(num_sentences)]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.load_subset(100)\n",
    "\n",
    "        for i in range(len(self.swedish_sentences)):\n",
    "            swedish_tokens = self.swedish_sentences[i].split()  # Splitting by whitespace\n",
    "            english_tokens = self.english_sentences[i].split()  # Splitting by whitespace\n",
    "\n",
    "            for token in swedish_tokens:\n",
    "                if token not in self.swedish_vocab:\n",
    "                    self.swedish_vocab[token] = len(self.swedish_vocab)\n",
    "\n",
    "            for token in english_tokens:\n",
    "                if token not in self.english_vocab:\n",
    "                    self.english_vocab[token] = len(self.english_vocab)\n",
    "    \n",
    "    def run_lang_model(self):\n",
    "        self.language_model = LanguageModel(self.ngram_order, self.english_sentences)\n",
    "\n",
    "    def init_trans_prob(self):\n",
    "        self.translation_probabilities = {\n",
    "            (swedish_word, english_word): 1.0 / len(self.english_vocab)\n",
    "            for swedish_word in self.swedish_vocab\n",
    "            for english_word in self.english_vocab\n",
    "        }\n",
    "\n",
    "    def train(self, num_iterations=10):\n",
    "        self.init_trans_prob()\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            print(self.translation_probabilities[(\"jag\", \"declare\")])\n",
    "            count_fe = {}\n",
    "\n",
    "            for swedish_word in self.swedish_vocab:\n",
    "                count_fe[swedish_word] = {}\n",
    "                for english_word in self.english_vocab:\n",
    "                    count_fe[swedish_word][english_word] = 0.0\n",
    "        \n",
    "            c_e = {}\n",
    "            for english_word in self.english_vocab:\n",
    "                    c_e[english_word] = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "            #Expectation\n",
    "            for k in range(len(self.swedish_sentences)): # For each sentence pair\n",
    "             swedish_tokens = self.swedish_sentences[k].split()  \n",
    "             english_tokens = self.english_sentences[k].split()\n",
    "\n",
    "             for swedish_word in swedish_tokens:  # For each swedish word\n",
    "                 total_sw = 0.0\n",
    "                 for english_word in english_tokens:\n",
    "                     total_sw += self.translation_probabilities[(swedish_word, english_word)]\n",
    "\n",
    "                 for english_word in english_tokens:     #For each english word\n",
    "                        delta = self.translation_probabilities[(swedish_word, english_word)] / total_sw  # Compute alignment prob\n",
    "                        count_fe[swedish_word][english_word] += delta  # Update pseudocount\n",
    "                        c_e[english_word] += delta  # Update pseudocount\n",
    "\n",
    "         # Maximization step (Reestimate probabilities)\n",
    "            for english_word in self.english_vocab:\n",
    "                for swedish_word in self.swedish_vocab:\n",
    "                    self.translation_probabilities[(swedish_word, english_word)] = count_fe[swedish_word][english_word] / c_e[english_word]\n",
    "\n",
    "IBM = IBMModel1('datasets/europarl-v7.sv-en.lc.sv', 'datasets/europarl-v7.sv-en.lc.en')\n",
    "IBM.preprocess_data()\n",
    "IBM.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
