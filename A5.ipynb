{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Warmup\n",
    "As a warmup, write code to collect statistics about word frequencies in the two languages. Print the 10 most frequent words in each language.\n",
    "\n",
    "If you're working with Python, using a CounterLinks to an external site. is probably the easiest solution.\n",
    "\n",
    "Let's assume that we pick a word completely randomly from the European parliament proceedings. According to your estimate, what is the probability that it is speaker? What is the probability that it is zebra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent(file_path):\n",
    "    sent = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences = re.split(r'\\s', line.strip())\n",
    "            for sentance in sentences:\n",
    "                if sentance.strip():\n",
    "                    sent.append(sentance.strip())\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_swe = \"datasets/europarl-v7.sv-en.lc.sv\"\n",
    "swe_sentance = extract_sent(file_path_swe)\n",
    "file_path_eng = \"datasets/europarl-v7.sv-en.lc.en\"\n",
    "eng_sentance = extract_sent(file_path_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_swe = Counter(swe_sentance)\n",
    "most_common_swe = counter_swe.most_common(10)\n",
    "for word, count in most_common_swe:\n",
    "   print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_eng = Counter(eng_sentance)\n",
    "most_common_eng = counter_eng.most_common(10)\n",
    "for word, count in most_common_eng:\n",
    "   print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_eng = len(counter_eng)\n",
    "speaker_counts = counter_eng['speaker']\n",
    "zebra_counts = counter_eng['zebra']\n",
    "the_counts = counter_eng['the']\n",
    "\n",
    "prob_speaker = speaker_counts/total_words_eng\n",
    "prob_zebra = zebra_counts/total_words_eng\n",
    "prob_the= the_counts / total_words_eng \n",
    "\n",
    "print(\"Probability of picking 'speaker' randomly:\", prob_speaker)\n",
    "print(\"Probability of picking 'zebra' randomly:\", prob_zebra)\n",
    "print(\"Probability of picking 'the' randomly:\", prob_the)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Language modeling\n",
    "We will now define a language model that lets us compute probabilities for individual English sentences.\n",
    "\n",
    "Implement a bigram language model as described in the lecture, and use it to compute the probability of a short sentence.\n",
    "What happens if you try to compute the probability of a sentence that contains a word that did not appear in the training texts? And what happens if your sentence is very long (e.g. 100 words or more)? Optionally, change your code so that it can handle these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "1. If a word in the text did not appear, the bigram probability would be 0. This might not accurately reflect the actual likelihood of the sentence occurring in real language usage. There are several ways to solve this, we will use add-1 smoothening. Add-1 smoothing involves adding a small constant (usually 1) to all observed counts before computing probabilities. This ensures that even unseen combinations of words have a non-zero probability.\n",
    "2. For very long sentances, the probability might become a very small number to the muliplication of probabilites. This could lead to that the floating-point arithmetic becomes less precise. To handle this we will use log probabilities instead of probabilities directly, as logarithms can help maintain numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigramModel:\n",
    "    def __init__(self, corpus) -> None:\n",
    "        self.corpus = corpus\n",
    "        self.vocab = set()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_prob = {}\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "\n",
    "        for word in self.corpus:\n",
    "            if word == '.':\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                sentence.append(word)\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                bigram = tuple(sentence[i:i+2])\n",
    "                self.bigram_counts[bigram] += 1\n",
    "\n",
    "        for bigram, count in self.bigram_counts.items():\n",
    "            self.bigram_prob[bigram] = count / self.unigram_counts[bigram[0]]\n",
    "\n",
    "    def sentence_prob(self, sentence):\n",
    "        words = sentence.split()\n",
    "        prob = 1.0\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = tuple(words[i:i+2])\n",
    "            if bigram in self.bigram_prob:\n",
    "                prob *= self.bigram_prob[bigram]\n",
    "            else:\n",
    "                prob *= 1 / (self.unigram_counts[bigram[0]] + len(self.vocab))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bigramModel(eng_sentance)\n",
    "sentence = \"i declare\"\n",
    "probability = model.sentence_prob(sentence)\n",
    "print(\"Probability of sentence '{}': {}\".format(sentence, probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Translation modeling. We will now estimate the parameters of the translation model P(f|e).\n",
    "\n",
    "Self-check: if our goal is to translate from some language into English, why does our conditional probability seem to be written backwards? Why don't we estimate P(e|f) instead?\n",
    "\n",
    "Write code that implements the estimation algorithm for IBM model 1. Then print, for either Swedish, German, or French, the 10 words that the English word european is most likely to be translated into, according to your estimate. It can be interesting to look at this list of 10 words and see how it changes during the EM iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ibm_model:\n",
    "    def __init__(self, english_corpus, foreign_corpus, num_iterations=10):\n",
    "        self.english_corpus = english_corpus\n",
    "        self.foreign_corpus = foreign_corpus\n",
    "        self.num_iterations = num_iterations\n",
    "        self.translation_probs = {}\n",
    "        self.initialize_translation_probs()\n",
    "\n",
    "    def initialize_translation_probs(self):\n",
    "        for foreign_sentence, english_sentence in zip(self.foreign_corpus, self.english_corpus):\n",
    "            foreign_words = foreign_sentence.split()\n",
    "            english_words = english_sentence.split()\n",
    "\n",
    "            for foreign_word in foreign_words:\n",
    "                if foreign_word not in self.translation_probs:\n",
    "                    self.translation_probs[foreign_word] = {}\n",
    "                    for english_word in english_words:\n",
    "                        self.translation_probs[foreign_word][english_word] = random.random()\n",
    "            \n",
    "# Preprocess English and foreign text\n",
    "english_corpus_preprocessed = []\n",
    "foreign_corpus_preprocessed = []\n",
    "\n",
    "for sentence in eng_sentance:\n",
    "    words = sentence.strip().split()\n",
    "    if words[-1] == 'NULL':\n",
    "        english_corpus_preprocessed.append(' '.join(words[:-1]) + ' NULL')\n",
    "    else:\n",
    "        english_corpus_preprocessed.append(' '.join(words))\n",
    "\n",
    "for sentence in swe_sentance:\n",
    "    words = sentence.strip().split()\n",
    "    if words[-1] == 'NULL':\n",
    "        foreign_corpus_preprocessed.append(' '.join(words[:-1]) + ' NULL')\n",
    "    else:\n",
    "        foreign_corpus_preprocessed.append(' '.join(words))\n",
    "\n",
    "# Initialize and train the IBM model\n",
    "ibm_model = ibm_model(english_corpus_preprocessed, foreign_corpus_preprocessed)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
