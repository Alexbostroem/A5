{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47932198471520016\n",
      "0.007710276573754092\n",
      "0.016504454337880652\n",
      "0.033074328597251894\n",
      "0.061048580398416784\n",
      "0.10112491681696638\n",
      "0.15055645469581197\n",
      "0.20434536360870698\n",
      "0.2577484921837751\n",
      "0.3077500379609673\n",
      "that did not happen .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, n, corpus, vocabulary):\n",
    "        self.n = n\n",
    "        self.ngrams = Counter()\n",
    "        self.total_ngrams = 0\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary_size = len(vocabulary)\n",
    "\n",
    "    def train(self):\n",
    "        for sentence in self.corpus:\n",
    "            tokens = sentence.split()\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i+self.n])\n",
    "                self.ngrams[ngram] += 1\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "    def log_probability(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        log_prob = 0.0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i+self.n])\n",
    "            prob = (self.ngrams[ngram] + 1) / (self.total_ngrams + self.vocabulary_size)  # Laplace smoothing\n",
    "            log_prob += math.log(prob)\n",
    "        return log_prob\n",
    "\n",
    "class IBMModel1:\n",
    "    def __init__(self, swedish_file, english_file):\n",
    "        self.swedish_file = swedish_file\n",
    "        self.english_file = english_file\n",
    "        self.swedish_sentences = []\n",
    "        self.english_sentences = []\n",
    "        self.swedish_vocab = {}\n",
    "        self.english_vocab = {}\n",
    "        self.translation_probabilities = {}\n",
    "        self.ngram_order = 2\n",
    "\n",
    "    def read_files(self):\n",
    "        with open(self.swedish_file, 'r', encoding='utf-8') as f:\n",
    "            self.swedish_sentences = f.readlines()\n",
    "        with open(self.english_file, 'r', encoding='utf-8') as f:\n",
    "            self.english_sentences = f.readlines()\n",
    "\n",
    "    def load_subset(self, num_sentences):\n",
    "        with open(self.swedish_file, 'r', encoding='utf-8') as f:\n",
    "            self.swedish_sentences = [next(f).strip() for _ in range(num_sentences)]\n",
    "        with open(self.english_file, 'r', encoding='utf-8') as f:\n",
    "            self.english_sentences = [next(f).strip() for _ in range(num_sentences)]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.load_subset(100)\n",
    "\n",
    "        for i in range(len(self.swedish_sentences)):\n",
    "            swedish_tokens = self.swedish_sentences[i].split()  # Splitting by whitespace\n",
    "            english_tokens = self.english_sentences[i].split()  # Splitting by whitespace\n",
    "\n",
    "            for token in swedish_tokens:\n",
    "                if token not in self.swedish_vocab:\n",
    "                    self.swedish_vocab[token] = len(self.swedish_vocab)\n",
    "\n",
    "            for token in english_tokens:\n",
    "                if token not in self.english_vocab:\n",
    "                    self.english_vocab[token] = len(self.english_vocab)\n",
    "\n",
    "    def init_trans_prob(self):\n",
    "        self.translation_probabilities = {\n",
    "            (swedish_word, english_word): random.random()\n",
    "            for swedish_word in self.swedish_vocab\n",
    "            for english_word in self.english_vocab\n",
    "        }\n",
    "\n",
    "    def train(self, num_iterations=10):\n",
    "        self.init_trans_prob()\n",
    "        self.language_model = LanguageModel(self.ngram_order, self.english_sentences, self.english_vocab)\n",
    "        self.language_model.train()\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            count_fe = {}\n",
    "            print(self.translation_probabilities[(\"europeiska\", \"european\")])\n",
    "\n",
    "            for swedish_word in self.swedish_vocab:\n",
    "                count_fe[swedish_word] = {}\n",
    "                for english_word in self.english_vocab:\n",
    "                    count_fe[swedish_word][english_word] = 0.0\n",
    "\n",
    "            c_e = {}\n",
    "            for english_word in self.english_vocab:\n",
    "                c_e[english_word] = 0.0\n",
    "                \n",
    "            # Expectation\n",
    "            for k in range(len(self.swedish_sentences)):  # For each sentence pair\n",
    "                swedish_tokens = self.swedish_sentences[k].split()\n",
    "                english_tokens = self.english_sentences[k].split()\n",
    "\n",
    "                for swedish_word in swedish_tokens:  # For each swedish word\n",
    "                    total_sw = sum(self.translation_probabilities.get((swedish_word, english_word), 0) for english_word in english_tokens)\n",
    "\n",
    "                    for english_word in english_tokens:  # For each english word\n",
    "                        try:\n",
    "                            delta = self.translation_probabilities[(swedish_word, english_word)] / total_sw  # Compute alignment prob\n",
    "                            count_fe[swedish_word][english_word] += delta  # Update pseudocount\n",
    "                            c_e[english_word] += delta  # Update pseudocount\n",
    "                        except ZeroDivisionError:\n",
    "                            pass  # Skip if total_sw is 0\n",
    "\n",
    "            # Maximization step (Reestimate probabilities)\n",
    "            for english_word in self.english_vocab:\n",
    "                for swedish_word in self.swedish_vocab:\n",
    "                    try:\n",
    "                        self.translation_probabilities[(swedish_word, english_word)] = count_fe[swedish_word][english_word] / c_e[english_word]\n",
    "                    except ZeroDivisionError:\n",
    "                        pass  # Skip if c_e[english_word] is 0\n",
    "\n",
    "    def log_probability(self, swedish_sentence, english_sentence):\n",
    "        log_prob = 0.0\n",
    "        for swedish_word in swedish_sentence.split():\n",
    "            for english_word in english_sentence:\n",
    "                try:\n",
    "                    prob = self.translation_probabilities[(swedish_word, english_word)]\n",
    "                    if prob <= 0:  # Handle non-positive probabilities\n",
    "                        log_prob += math.log(1e-10)  # Add small epsilon instead of log(prob)\n",
    "                    else:\n",
    "                        log_prob += math.log(prob)\n",
    "                except KeyError:\n",
    "                    pass  # Skip if translation probability is not found\n",
    "        return log_prob   \n",
    "              \n",
    "    def get_top_10(self, english_word, number_of_trans=10):\n",
    "        translation = []\n",
    "        for swedish_word, trans_prob in self.translation_probabilities.items():\n",
    "            if swedish_word[1] == english_word:\n",
    "                translation.append((swedish_word[0], trans_prob))\n",
    "        translation.sort(key=lambda x: x[1], reverse=True)\n",
    "        return translation[:number_of_trans]\n",
    "\n",
    "    def decoder(self, swedish_sentence):\n",
    "        max_log_prob = float('-inf')\n",
    "        best_translation = None\n",
    "\n",
    "        for english_sentence in self.english_sentences:\n",
    "            p_e = self.language_model.log_probability(english_sentence)\n",
    "            p_f_e = self.log_probability(swedish_sentence, english_sentence)\n",
    "            joint_log_prob = p_e + p_f_e\n",
    "\n",
    "            if joint_log_prob > max_log_prob:\n",
    "                max_log_prob = joint_log_prob\n",
    "                best_translation = english_sentence\n",
    "\n",
    "        return best_translation\n",
    "\n",
    "# Example usage:\n",
    "IBM = IBMModel1('datasets/europarl-v7.sv-en.lc.sv', 'datasets/europarl-v7.sv-en.lc.en')\n",
    "IBM.preprocess_data()\n",
    "IBM.train()\n",
    "\n",
    "swedish_sentence = 'jag tror att det är ett bra sätt .'\n",
    "english_translation = IBM.decoder(swedish_sentence)\n",
    "print(english_translation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top translations for 'european' in Swedish:\n",
      "europeiska 0.35301184881870934\n",
      "för 0.15145843834191944\n",
      "i 0.12903371480804549\n",
      "de 0.0575277424221576\n",
      "på 0.04495331308244658\n",
      "med 0.036299816522218864\n",
      "ett 0.029792549882698856\n",
      ". 0.02837076835401436\n",
      "av 0.023968595840000516\n",
      "samt 0.023156696006807215\n"
     ]
    }
   ],
   "source": [
    "top_translations = IBM.get_top_10(\"european\")\n",
    "print(\"Top translations for 'european' in Swedish:\")\n",
    "for swedish_word, trans_prob in top_translations:\n",
    "    print(swedish_word,trans_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
